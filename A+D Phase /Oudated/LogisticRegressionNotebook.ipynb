{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549c898-657e-4e9d-8b43-c299bc0774b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic Regression\n",
    "This Logistic regression complies with L2 Regularizátion, where it includes the sigmoid function, Cost function with\n",
    "L2 Regularizátion, Gradient computation, Gradient descent optimization, Model prediction, Evaluation and visualiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63797fff-d22e-4a1d-bf34-18b5b973de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "On the following jupyter cell we import the libraries which are utilized for the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cad46-5099-451d-a18a-4546fcdc8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c449e58-a6b6-4077-9663-5912de425045",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid Function, which this function will convert every value into 0 and 1, which it helps us \n",
    "on interpreting it into a probability. Where we use the np.clip(z, 500, 500) in order to avoid overflow when z \n",
    "is too large or way too negative for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac9f73-4f46-4606-820f-5a0d91a72e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    z = np.array(z, dtype=float)\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4f0ca-f85b-4d46-bf54-50a3a24828c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cost Function, This function will measure how our model is far from the models prediction, in order to penalize \n",
    "the large weights to prevent overfitting we use the regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a71304-b552-473d-afdb-7fffabe1c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta, lambda_=0.0):\n",
    "    m = len(y)\n",
    "    h = logistic(X.dot(theta))  \n",
    "    h = np.clip(h, 1e-15, 1 - 1e-15)  \n",
    "    cost = - (1/m) * (y.T.dot(np.log(h)) + (1 - y).T.dot(np.log(1 - h)))\n",
    "    reg = (lambda_ / (2*m)) * np.sum(theta[1:]**2)  \n",
    "    return cost + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b10d98-aa77-40e3-85bc-0bfefa163c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient, Which will tell us how to change the weights in case we would want to reduce the error, where \n",
    "we compute the difference between the predictions between h and actual y. Then we will applu the vectorized gradient formula\n",
    "and also the L2 Regularization which is useful to discourage the large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30553d8d-9ded-4dbe-a3b8-f34b169f24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, theta, lambda_=0.0):\n",
    "    m = len(y)\n",
    "    h = logistic(X.dot(theta))\n",
    "    grad = (1/m) * (X.T.dot(h - y))\n",
    "    grad[1:] += (lambda_/m) * theta[1:]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d3176-d2f4-4495-b674-bc153e20b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent, where here the fucntionw will train the model, where it moves the theta in the opposite direction \n",
    "of the gradient in order reduce the error. The learning rate will control how big the steps are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25316641-45ad-4ed1-83e9-81890ff5c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations, lambda_=0.0):\n",
    "    cost_history = []\n",
    "    for _ in range(iterations):\n",
    "        grad = gradient(X, y, theta, lambda_)\n",
    "        theta -= alpha * grad\n",
    "        cost_history.append(compute_cost(X, y, theta, lambda_))\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3bb09-3f25-4117-a03a-93d51c9804c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction Function, which we will turn the probabilities into class labels, basically where if we have more or \n",
    "equal than 0.5 it will be class 1, otherwise it will be 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93086857-e75a-4e7e-94d7-f624881cd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta, threshold=0.5):\n",
    "    probs = logistic(X.dot(theta))\n",
    "    return (probs >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcbfc7-7373-4737-858e-7cd789ab1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "Load Dataset, its time to load the dataset which we have, we will use the dropna() which will help us fropping the \n",
    "rows which have NaN in order to avoid any time of error in maths. Which will keep the numeric features, since \n",
    "when talking about logistic regression we mention that it will work onlyw ith numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16436cac-413f-4f44-9a74-5876391d34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_weather_data_2001-2021.csv')\n",
    "data = data.dropna()\n",
    "data = data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70fc33-3bac-4081-a662-82554474192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create Labels, Where the last numeric column will be turned into the binary classification target, where\n",
    "1 if the value is above the median and otherwise is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fa608-3f61-4ba3-ba79-0dc328ff7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns.tolist()\n",
    "target = cols[-1]\n",
    "data['label'] = (data[target] >= data[target].median()).astype(int)\n",
    "feature_cols = cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a7ebc-91be-4d89-b5bd-cc12584cd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scale the Features and Split the Dataset, by using the StandardScaler we would be able to place the features on the\n",
    "same scale which is mean0 and std 1 helping the gradient descent converge way faster. Where the columns of 1 will\n",
    "be added to X to Account for the bias term and after we would split the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc060bd0-7853-44d2-a8ff-4cbc744edc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data[feature_cols].values)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['label'].values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec10c25-9016-401a-b214-948b6c0463da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train the model, as it name explains it will train the model, initializing the weights(theta) to zeros, then running\n",
    "the gradient descent for 3000 steps using alpha = 0.1 as the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa3e84-2ed9-4ae6-94e7-89d84c29f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "alpha = 0.1\n",
    "iterations = 3000\n",
    "lambda_ = 1.0\n",
    "\n",
    "theta, cost_history = gradient_descent(X_train, y_train, theta, alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff32fc2-62cd-427c-81e5-bd592405822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluating the performance, it will give us the check on how well the model will fit, which we would conclude\n",
    "Final Cost (The lower the better) and accuracy on the training and test date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c39f1-ec86-44c6-aef3-1642502c13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final cost: {cost_history[-1]:.4f}\")\n",
    "print(f\"Training accuracy: {np.mean(predict(X_train, theta) == y_train):.4f}\")\n",
    "print(f\"Testing accuracy : {np.mean(predict(X_test, theta) == y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f86788-ff42-4b16-9726-cfd6a8953c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detailed Classification metrics, these are the metrics which will show us the \n",
    "- Precision = Of predictive positives which is how many were correct \n",
    "- Recall = Of the actual positives we have how many we have found \n",
    "- F1 Score = Whihc si the harmonic mean of the precision and recall "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
